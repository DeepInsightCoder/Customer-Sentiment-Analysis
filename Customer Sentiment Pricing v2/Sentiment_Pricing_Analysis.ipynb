{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9a4a04e-802f-48b1-9c24-bde4ac5ce538",
   "metadata": {},
   "source": [
    "# Data Loading and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a08dc957-1302-429b-bf5a-68428f32d87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load datasets\n",
    "sentiment140_data = pd.read_csv('new_train_data_s140.csv') \n",
    "trustpilot_reviews_data = pd.read_csv('trust_pilot_reviews_data_2022_06.csv') \n",
    "twitter_data = pd.read_csv('Twitter Scraping Tweets Dataset.csv')\n",
    "reviews_data = pd.read_csv('Reviews.csv') \n",
    "ratings_beauty_data = pd.read_csv('ratings_beauty.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5f3b15d-7731-4ad1-bc2c-a8efc807bfee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment140 Data:\n",
      "   Polarity          Id                          Date     Query      User  \\\n",
      "0         0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY  mattycus   \n",
      "1         0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   ElleCTF   \n",
      "2         0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY    Karoli   \n",
      "3         0  1467811372  Mon Apr 06 22:20:00 PDT 2009  NO_QUERY  joy_wolf   \n",
      "4         0  1467811592  Mon Apr 06 22:20:03 PDT 2009  NO_QUERY   mybirch   \n",
      "\n",
      "                                                Text  \n",
      "0  @Kenichan I dived many times for the ball. Man...  \n",
      "1    my whole body feels itchy and like its on fire   \n",
      "2  @nationwideclass no, it's not behaving at all....  \n",
      "3                      @Kwesidei not the whole crew   \n",
      "4                                        Need a hug   \n",
      "\n",
      "Trustpilot Reviews Data:\n",
      "        name                 company_url  \\\n",
      "0  Poundshop  https://www.poundshop.com/   \n",
      "1  Poundshop  https://www.poundshop.com/   \n",
      "2  Poundshop  https://www.poundshop.com/   \n",
      "3  Poundshop  https://www.poundshop.com/   \n",
      "4  Poundshop  https://www.poundshop.com/   \n",
      "\n",
      "                                      trustpilot_url  \\\n",
      "0  https://uk.trustpilot.com/review/www.poundshop...   \n",
      "1  https://uk.trustpilot.com/review/www.poundshop...   \n",
      "2  https://uk.trustpilot.com/review/www.poundshop...   \n",
      "3  https://uk.trustpilot.com/review/www.poundshop...   \n",
      "4  https://uk.trustpilot.com/review/www.poundshop...   \n",
      "\n",
      "                                         description       author_name  \\\n",
      "0  The home for top brands, amazing value! We hav...             Colin   \n",
      "1  The home for top brands, amazing value! We hav...  Mr Paul Harrison   \n",
      "2  The home for top brands, amazing value! We hav...          Mrs Dean   \n",
      "3  The home for top brands, amazing value! We hav...        Mrs Barlow   \n",
      "4  The home for top brands, amazing value! We hav...   Graham Bradbury   \n",
      "\n",
      "                              review_title  \\\n",
      "0              Lowest price in the country   \n",
      "1  got it on time they took notice that Iâ€¦   \n",
      "2                        Delivered on time   \n",
      "3            Quick and efficient service..   \n",
      "4            Excellent service packed well   \n",
      "\n",
      "                                         review_text  rating  \\\n",
      "0  Lowest price in the country for what we wanted...       5   \n",
      "1  got it on time they took notice that I let the...       5   \n",
      "2  Delivered on time. Products as described, pack...       5   \n",
      "3               Quick and efficient service..Pleased       5   \n",
      "4     Excellent service packed well. Really pleased.       5   \n",
      "\n",
      "                reviewed_at                               uniq_id scraped_at  \n",
      "0  2022-06-18T12:08:09.000Z  7e0edfe9-7251-5561-acc8-34f7d3ab79aa   06/18/22  \n",
      "1  2022-06-18T11:48:20.000Z  7e0edfe9-7251-5561-acc8-34f7d3ab79aa   06/18/22  \n",
      "2  2022-06-18T11:32:52.000Z  7e0edfe9-7251-5561-acc8-34f7d3ab79aa   06/18/22  \n",
      "3  2022-06-18T11:29:27.000Z  7e0edfe9-7251-5561-acc8-34f7d3ab79aa   06/18/22  \n",
      "4  2022-06-18T11:22:37.000Z  7e0edfe9-7251-5561-acc8-34f7d3ab79aa   06/18/22  \n",
      "\n",
      "Twitter Data:\n",
      "   Unnamed: 0                                 user_name  \\\n",
      "0           0                                Aravindh S   \n",
      "1           1                            Gbest Bulk SMS   \n",
      "2           2                       Kalyanashis Mahanty   \n",
      "3           3  Network Palava - Free, Cheap Data Daily.   \n",
      "4           4                                 JayRoy ðŸ‡®ðŸ‡³   \n",
      "\n",
      "                    user_location  \\\n",
      "0               Tamil Nadu, India   \n",
      "1                   Abuja Nigeria   \n",
      "2  Barabhum ,  West Bengal, India   \n",
      "3                  Lagos, Nigeria   \n",
      "4                             NaN   \n",
      "\n",
      "                                    user_description  user_verified  \\\n",
      "0                         Nemophilist ðŸŒŽ 90's KID ðŸ˜‰ ðŸ˜œ          False   \n",
      "1  Providers of Bulk SMS | Data Bundle | Airtime ...          False   \n",
      "2  School teacher by profession and I am everythi...          False   \n",
      "3  Get Cheap, Free Browsing Solutions at https://...          False   \n",
      "4         Proud Indian \\n\\n(RT's r not endorsements)          False   \n",
      "\n",
      "               date                                               text  \\\n",
      "0  14-03-2022 12:38  And I am hearing new stories that within a sma...   \n",
      "1  14-03-2022 12:21  Do U have excess Airtime in ur line and will l...   \n",
      "2  14-03-2022 12:05  @airtelindia @Airtel_Presence bye bye for now....   \n",
      "3  14-03-2022 11:59  Working NapsternetV Configuration Files Downlo...   \n",
      "4  14-03-2022 11:55  @airtelnews @airtelindia @Airtel_Presence @air...   \n",
      "\n",
      "  hashtags               source    label  \n",
      "0      NaN   Twitter for iPhone        0  \n",
      "1      NaN  Twitter for Android  neutral  \n",
      "2      NaN  Twitter for Android        0  \n",
      "3  ['mtn']    Revive Social App  neutral  \n",
      "4      NaN      Twitter Web App        0  \n",
      "\n",
      "Reviews Data:\n",
      "   Id   ProductId          UserId                      ProfileName  \\\n",
      "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
      "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
      "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
      "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
      "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
      "\n",
      "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
      "0                     1                       1      5  1303862400   \n",
      "1                     0                       0      1  1346976000   \n",
      "2                     1                       1      4  1219017600   \n",
      "3                     3                       3      2  1307923200   \n",
      "4                     0                       0      5  1350777600   \n",
      "\n",
      "                 Summary                                               Text  \n",
      "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
      "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
      "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
      "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
      "4            Great taffy  Great taffy at a great price.  There was a wid...  \n",
      "\n",
      "Ratings Beauty Data:\n",
      "           UserId   ProductId  Rating   Timestamp\n",
      "0  A39HTATAQ9V7YF  0205616461     5.0  1369699200\n",
      "1  A3JM6GV9MNOF9X  0558925278     3.0  1355443200\n",
      "2  A1Z513UWSAAO0F  0558925278     5.0  1404691200\n",
      "3  A1WMRR494NWEWV  0733001998     4.0  1382572800\n",
      "4  A3IAAVS479H7M7  0737104473     1.0  1274227200\n"
     ]
    }
   ],
   "source": [
    "# Display first few rows of each dataset to check\n",
    "print(\"Sentiment140 Data:\")\n",
    "print(sentiment140_data.head())\n",
    "print(\"\\nTrustpilot Reviews Data:\")\n",
    "print(trustpilot_reviews_data.head())\n",
    "print(\"\\nTwitter Data:\")\n",
    "print(twitter_data.head())\n",
    "print(\"\\nReviews Data:\")\n",
    "print(reviews_data.head())\n",
    "print(\"\\nRatings Beauty Data:\")\n",
    "print(ratings_beauty_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6e21bb-7376-480d-8f2e-e0e295af7b21",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "**Cleaning Text Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fa8336b-9aeb-4887-8ee9-bc679f62cfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentiment140(data):\n",
    "    data = data[['Polarity', 'Text']]\n",
    "    return data\n",
    "\n",
    "sentiment140_preprocessed = preprocess_sentiment140(sentiment140_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5097d138-5369-4970-83d5-cfc93b7e7875",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Global\\AppData\\Local\\Temp\\ipykernel_2764\\32807683.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data.rename(columns={'review_text': 'Text', 'rating': 'Polarity'}, inplace=True)\n",
      "C:\\Users\\Global\\AppData\\Local\\Temp\\ipykernel_2764\\32807683.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['Polarity'] = data['Polarity'].apply(lambda x: 1 if x > 3 else 0)\n"
     ]
    }
   ],
   "source": [
    "def preprocess_trustpilot(data):\n",
    "    data = data[['review_text', 'rating']]\n",
    "    data.rename(columns={'review_text': 'Text', 'rating': 'Polarity'}, inplace=True)\n",
    "    data['Polarity'] = data['Polarity'].apply(lambda x: 1 if x > 3 else 0)\n",
    "    return data\n",
    "\n",
    "trustpilot_preprocessed = preprocess_trustpilot(trustpilot_reviews_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1135ee10-c1c4-4ce9-882d-0928b943387a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Global\\AppData\\Local\\Temp\\ipykernel_2764\\1963007449.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data.rename(columns={'text': 'Text', 'label': 'Polarity'}, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "def preprocess_twitter(data):\n",
    "    data = data[['text', 'label']]\n",
    "    data.rename(columns={'text': 'Text', 'label': 'Polarity'}, inplace=True)\n",
    "    return data\n",
    "\n",
    "twitter_preprocessed = preprocess_twitter(twitter_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba8b8c7a-4500-4bd7-8791-185c442172d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Global\\AppData\\Local\\Temp\\ipykernel_2764\\1943644809.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['Polarity'] = data['Score'].apply(lambda x: 1 if x > 3 else 0)\n"
     ]
    }
   ],
   "source": [
    "def preprocess_reviews(data):\n",
    "    data = data[['Text', 'Score']]\n",
    "    data['Polarity'] = data['Score'].apply(lambda x: 1 if x > 3 else 0)\n",
    "    return data[['Polarity', 'Text']] \n",
    "reviews_preprocessed = preprocess_reviews(reviews_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee6f51f0-9759-40da-ad90-729db471c5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_ratings_beauty(data):\n",
    "    data['Polarity'] = data['Rating'].apply(lambda x: 1 if x > 3 else 0)\n",
    "    return data[['Polarity']]\n",
    "\n",
    "ratings_beauty_preprocessed = preprocess_ratings_beauty(ratings_beauty_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848017a9-3279-4249-9fff-016ffe89f6ef",
   "metadata": {},
   "source": [
    "**Save the Preprocessed Datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a6e7fe4-131f-4eb6-b7af-2bc46783e26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment140_preprocessed.to_csv('sentiment140_preprocessed.csv', index=False)\n",
    "trustpilot_preprocessed.to_csv('trustpilot_preprocessed.csv', index=False)\n",
    "twitter_preprocessed.to_csv('twitter_preprocessed.csv', index=False)\n",
    "reviews_preprocessed.to_csv('reviews_preprocessed.csv', index=False)\n",
    "ratings_beauty_preprocessed.to_csv('ratings_beauty_preprocessed.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d67a25d-b3ef-4f92-802c-d3da6f00fe13",
   "metadata": {},
   "source": [
    "#  Data Combination and Shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07de002c-3fa6-4fdc-8e84-3826e61baa3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Polarity', 'Text'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Combine datasets (only those with text)\n",
    "combined_data = pd.concat([\n",
    "    sentiment140_preprocessed, \n",
    "    trustpilot_preprocessed, \n",
    "    twitter_preprocessed, \n",
    "    reviews_preprocessed\n",
    "], ignore_index=True)\n",
    "\n",
    "# Shuffle combined data\n",
    "combined_data = combined_data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Check the columns of the combined dataset\n",
    "print(combined_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eab8f248-ed04-4786-a0bc-8dbb2b591624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values before handling:\n",
      "Polarity    0\n",
      "Id          0\n",
      "Date        0\n",
      "Query       0\n",
      "User        0\n",
      "Text        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for and handle missing values\n",
    "print(\"Missing values before handling:\")\n",
    "print(sentiment140_data.isnull().sum())\n",
    "\n",
    "# Drop rows with NaN values\n",
    "sentiment140_data.dropna(inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b83575-6275-4b04-9b96-9cf6f2a21016",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2597ad9e-177c-4f53-9fe6-a736290bba4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Vectorize the text data\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X = vectorizer.fit_transform(combined_data['Text'])\n",
    "\n",
    "# Labels for sentiment\n",
    "y = combined_data['Polarity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a85d1d63-4f79-495e-b639-3ff67ac9e73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure y is numeric\n",
    "y = pd.to_numeric(y, errors='coerce')\n",
    "\n",
    "# Drop rows with missing target values (y)\n",
    "X = X[y.notnull()]\n",
    "y = y[y.notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc7f2c2-ad34-4dbf-9b59-d3ca776539c8",
   "metadata": {},
   "source": [
    "# Train-Test Split and Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "696a9ee4-fbf3-4b80-8c18-9a866765f4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c5ec9142-ee2a-4f6e-9a9d-74182b0d7fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Global\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Train the Logistic Regression model\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "lr_accuracy = lr.score(X_test, y_test)\n",
    "print(f\"Model Accuracy: {lr_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e35a571-3bd9-42ff-9401-473fdaa712d8",
   "metadata": {},
   "source": [
    "# Save the Model and Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "87d656c8-be0d-45a6-bca8-926cd9817dc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tfidf_vectorizer.pkl']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the trained model and vectorizer\n",
    "joblib.dump(lr, 'sentiment_analysis_model.pkl')\n",
    "joblib.dump(vectorizer, 'tfidf_vectorizer.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
